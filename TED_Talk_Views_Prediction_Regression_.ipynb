{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "u3PMJOP6ngxN",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "Xj3U_gcMTzax",
        "g-ATYxFrGrvw",
        "8yEUt7NnHlrM",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "49K5P_iCpZyH",
        "kLW572S8pZyI",
        "578E2V7j08f6",
        "67NQN5KX2AMe",
        "Hlsf0x5436Go",
        "c49ITxTc407N",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "C74aWNz2AliB",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "ArJBuiUVfxKd",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhagyashridhavale/Capstone-project-I-Hotel-booking-analysis/blob/main/TED_Talk_Views_Prediction_Regression_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - \n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name-**           - Bhagyashri Ramesh Dhavale\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the summary here within 500-600 words.\n",
        "\n",
        "TED is a nonprofit devoted to spreading ideas, usually in the form of short, powerful talks (18 minutes or less). TED began in 1984 as a conference where Technology, Entertainment and Design converged, and today covers almost all topics — from science to business to global issues — in more than 100 languages.\n",
        "\n",
        "In this talk, the speaker discusses the importance of self-care and how it can improve our lives. Self-care is a way to take care of ourselves and our mental health, and it can help us be more productive and successful in both our personal and professional lives. The speaker encourages us to make time for ourselves and to prioritize our own needs, even if it means taking a break from work or saying no to social activities. The speaker also emphasizes the importance of self-compassion and how it can help us cope with difficult times. Self-care is a necessary part of life and it can help us be our best selves."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here.\n",
        "\n",
        "https://github.com/bhagyashridhavale/TED-Talk-Views-Prediction/tree/main"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TED is devoted to spreading powerful ideas on just about any topic . These datasets contain over 4,000 TED talks including transcripts in many languages.Founded in 1984 by Richard Salman as a nonprofit organization that aimed at bringing experts from the fields of Technology, Entertainment, and Design together, TED Conferences have gone on to become the Mecca of ideas from virtually all walks of life. As of 2015,TED and its sister TEDx chapters have published more than 2000 talks for free consumption by the masses and its speaker list boasts of the likes of Al Gore, Jimmy Wales, Shahrukh Khan,and Bill Gates. The main objective is to build a predictive model, which could help in predicting the views of the videos uploaded on the TEDx website."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn import neighbors\n",
        "from sklearn.svm import SVR\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn import ensemble\n",
        "from wordcloud import WordCloud,ImageColorGenerator\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_regression\n",
        "\n",
        "# ignoring warnings.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8_bzo0CNKkpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Machine learning project/data_ted_talks.csv\")"
      ],
      "metadata": {
        "id": "v2iZwFT5KqJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.rcParams['figure.figsize'] = (15, 10)\n",
        "sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='BuPu')"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looking how many values are missing.\n",
        "\n",
        "def missing_zero_values_table(df):\n",
        "  zero_val = (df == 0.00).astype(int).sum(axis=0)\n",
        "  mis_val = df.isnull().sum()\n",
        "  mis_val_percent = 100 * df.isnull().sum()/len(df)\n",
        "  mz_table = pd.concat([zero_val,mis_val,mis_val_percent],axis=1)\n",
        "  mz_table = mz_table.rename(\n",
        "  columns = {0 : 'Zero Values' , 1 : 'Missing Values' , 2 : '% of Total Values'})\n",
        "  mz_table['Total Zero Missing Values'] = mz_table['Zero Values'] + mz_table['Missing Values']\n",
        "  mz_table['% Total Zero Missing Values'] = 100 * mz_table[\"Total Zero Missing Values\"]/len(df)\n",
        "  mz_table['Data Type'] = df.dtypes\n",
        "  mz_table = mz_table[\n",
        "      mz_table.iloc[:,1] !=0].sort_values(\n",
        "  '% of Total Values',ascending=False).round(1)\n",
        "  print (\" Your selected dataframe has \" + str (df.shape[1]) + \" columns and \" + str (df.shape[0]) + \" Rows.\\n \"\n",
        "      \" There are \" + str (mz_table.shape[0]) +\n",
        "          \" columns that have missing values . \" )\n",
        "#    mz_table.to_excel('D:/sampledata/missing_and_zero_values.xlsx',freeze_panes=(1,0),index=False)\n",
        "  return mz_table\n",
        "missing_zero_values_table(df)"
      ],
      "metadata": {
        "id": "yT92iypGt-8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "***The TEDx dataset is a collection of talks from TEDx events, which are independently organized TED events. The talks in the dataset are provided by TEDx organizers, speakers, and partners, and feature a range of topics, including science, technology, art, education, and more. The dataset includes information about the talk, such as the title, speaker, event, and video URL. Additionally, the dataset includes transcripts and time-coded summaries of the talks, which can be used to understand the topics discussed in each talk.***\n",
        "\n",
        "***1) The above dataset has 4005 rows and 19 columns.***\n",
        "\n",
        "***2) zero(0) duplicate values.***\n",
        "\n",
        "***3)There are 5 columns that have mising values.***\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "Talk id= Talk id.\n",
        "\n",
        "Title = The title of the talk.\n",
        "\n",
        "Speaker_1 = The first named speaker of the talk.\n",
        "\n",
        "All speakers = Number of the all speakers.\n",
        "\n",
        "Occupations = Occupation of the speakers.\n",
        "\n",
        "About speakers = All information about speaker.\n",
        "\n",
        "Recorded date= Recording date.\n",
        "\n",
        "Published date = Publication of the talk on TED.com\n",
        "\n",
        "Views = The number of views on the talk.\n",
        "\n",
        "Event = The TEDx event where the talk took place.\n",
        "\n",
        "Available language = English\n",
        "\n",
        "Comments = The number of languages in which the talk is available.\n",
        "\n",
        "Duration = Duration of the talk in seeconds.\n",
        "\n",
        "Topics = Topic of the talk.\n",
        "\n",
        "Related talk = A list of dictionaries of talks.\n",
        "\n",
        "Url = The URL of the talk."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No.of unique values in\",i,\"is\",df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Outlier\n",
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()"
      ],
      "metadata": {
        "id": "jDrXdNz3O88s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see type of published_date\n",
        "type(\"published_date\")"
      ],
      "metadata": {
        "id": "ZZjjK0jKPDVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see datatypes of all the features\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "D_eagAvkPHWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting object data type to datetime for published_date\n",
        "import datetime\n",
        "df['published_date'] = df['published_date'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\"))"
      ],
      "metadata": {
        "id": "e2cBjGdEPPRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes\n"
      ],
      "metadata": {
        "id": "0BedgPQaPVfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting release day, month and year from the published date column\n",
        "df['release_day'] = df['published_date'].apply(lambda x: x.weekday())\n",
        "df['release_month']=df['published_date'].apply(lambda x: x.month)\n",
        "df['release_year'] = df['published_date'].apply(lambda x: x.year)"
      ],
      "metadata": {
        "id": "5ZVZsfpMPbcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Encoding of release_day column with corresponding week day name\n",
        "week_day={0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
        "df['release_day']=df['release_day'].map(week_day)"
      ],
      "metadata": {
        "id": "LwqDpXEAPiZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding of release_month column with corresponding month name\n",
        "month_dict={1:'Jan',2:'Feb',3:'March',4:'April',5:'May',6:'June',7:'July',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'}\n",
        "df['release_month']=df['release_month'].map(month_dict)"
      ],
      "metadata": {
        "id": "bxM2zXE6Pnhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1) comments, duration, talk id, and views this data has huge outliners.***\n",
        "\n",
        "***2) I applied strtime on publish date.***"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "ax = sns.barplot(x=\"views\", y=\"speaker_1\", data=df.sort_values('views',ascending=False)[:20])"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***visualization the data in the form of a bar graph,to understand the data better.***"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***It appears that most viewers are interested in motivational and educational talks that focus mainly on self-improvement and happiness.***"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Positive business impact=TED talk analysis can help to identify potential influencers who can help to spread the word about a business or product.***\n",
        "\n",
        "***sir ken robinson has more views on there video,means more people watch video,it has a good sign.***\n",
        "\n"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "pop_df = df.nlargest(10,[\"views\"])\n",
        "pop_df[[\"speaker_1\",\"views\",\"title\"]]"
      ],
      "metadata": {
        "id": "RTHwHcrDQuLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar Plot for Top 10 Views multivariate\n",
        "# Plot for most popular videos\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.title(\"Speaker's of most popular video\")\n",
        "sns.barplot(x=\"speaker_1\",y=\"views\",data=pop_df)"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1) seaborn displot represents the overall distribution of continous data    variables.***\n",
        "\n",
        "***2) To check the distribution of the data and then if views and duration are anywhere related.***"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1) Do schools kill creativity is the title of sir ken robinson video.***\n",
        "\n",
        "***2) 65051954 this much views on video.***"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***People like to watch and listen videos,and people gainig new knowledge.***"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Plot for most popular videos\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.title(\"Speaker's of most popular video\")\n",
        "sns.barplot(x=\"speaker_1\",y=\"views\",data=pop_df)"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Countplot chart are used to display the frequency of given variable. They are ideal for displaying the number of occurrences of a given value in a dataset.They are also useful for comparing the frequency of different values in a dataset , or for comparing the frequency of a given values across different datasets.Countplot chart are easy to interpret and understand , making them a popular choice for visualizing data.***"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1) sir ken robinson video is more popular.***\n",
        "\n",
        "***2) Bill gates video has less views compare to sir ken robinson.***\n"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Compare to videos views we know the audience interest,more preferable topics.***"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# most popular speaker univariate\n",
        "pop_speaker_df=df.groupby('speaker_1').agg({'views' : 'sum'}).nlargest(10,['views'])\n",
        "pop_speaker_df.reset_index(inplace=True)\n",
        "pop_speaker_df"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.title(\"Total average views of speakers on their talks\")\n",
        "sns.barplot(x='speaker_1',y='views',data=pop_speaker_df)"
      ],
      "metadata": {
        "id": "Iugibq7FRI8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***To show the count of average view of speaker on their talk.***"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***I can say that all talks are equally important.***\n",
        "\n",
        "***Alex gendle is the most popular speaker.***"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Yes it has postive impact ,here Alex gendle and robin sir has more view ,and it will benefited.***"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "#Most frequent event category univariate\n",
        "df[\"event\"].nunique()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_event_df=df[['event','views']].groupby('event').agg({'views' : 'count'}).nlargest(10,['views'])\n",
        "freq_event_df.reset_index(inplace=True)\n",
        "freq_event_df"
      ],
      "metadata": {
        "id": "zL1sqBD8RZjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "plt.title('Most frequent event category')\n",
        "sns.barplot(x='event',y='views',data=freq_event_df)"
      ],
      "metadata": {
        "id": "6MR7r56XRc70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***count plot show the frequency very well.***\n",
        "\n",
        "***It shows the relationship between a numeric and categoric variable.***"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here\n",
        "\n",
        "***1) Insight this graph i found TED-ed is the most frequent event category.***\n",
        "\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***This plot shows some negative impact because except ted-ed other has very low views.***"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# most Popular event category\n",
        "pop_event_df = df[[\"event\",\"views\"]].groupby(\"event\").agg({\"views\":\"sum\"}).nlargest(10,[\"views\"])\n",
        "pop_event_df.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "plt.title('Top event category according to average views')\n",
        "sns.barplot(x='event',y='views',data=pop_event_df)"
      ],
      "metadata": {
        "id": "hl5ob13ARp-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Box plot are used to show distributions of numeric data values.especially when you want to compare them with multile groups.***"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Ted-ed is the Top event category according to average views.***"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***To help of this plot i know some has positive impact and some has negative impact.***"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "all_words = ' '.join([text for text in df['topics']])\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Word clouds are typically used as a tool for processing, analyzing and disseminating qualitative sentiment data.***"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Significant textual data points can be highlighted using a word cloud.it shows data in highlighted like ted id,global issues etc.***"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Helps to point out the areas or scopes of popularity and recognition from the viewpoint and perspective of the audience.Display it in the office to communicate them to guests in a fun and visual way.***"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "all_words = ' '.join([text for text in df['title']])\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Word clouds are typically used as a tool for processing, analyzing and disseminating qualitative sentiment data.***"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Significant textual data points can be highlighted using a word cloud.it shows data in highlighted like ted id,global issues etc.***"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Helps to point out the areas or scopes of popularity and recognition from the viewpoint and perspective of the audience.Display it in the office to communicate them to guests in a fun and visual way.***"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Extracting release day, month and year from the published date column\n",
        "df['release_day'] = df['published_date'].apply(lambda x: x.weekday())\n",
        "df['release_month']=df['published_date'].apply(lambda x: x.month)\n",
        "df['release_year'] = df['published_date'].apply(lambda x: x.year)"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding of release_day column with corresponding week day name\n",
        "week_day={0:'Monday',1:'Tuesday',2:'Wednesday',3:'Thursday',4:'Friday',5:'Saturday',6:'Sunday'}\n",
        "df['release_day']=df['release_day'].map(week_day)"
      ],
      "metadata": {
        "id": "N3Xs8tQLSP33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the day of the month having maximum release date\n",
        "freq_rel_day=df[['release_day','views']].groupby('release_day').agg({'views' : 'count'})\n",
        "freq_rel_day=freq_rel_day.sort_values('views',ascending=False)\n",
        "freq_rel_day.reset_index(inplace=True)\n",
        "freq_rel_day"
      ],
      "metadata": {
        "id": "UrctonWvSTah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extarcting release_day and count of views\n",
        "freq_rel_day=df[['release_day','views']].groupby('release_day').agg({'views' : 'count'})\n",
        "freq_rel_day=freq_rel_day.sort_values('views',ascending=False)\n",
        "freq_rel_day.reset_index(inplace=True)\n",
        "freq_rel_day"
      ],
      "metadata": {
        "id": "VyFoPjWvSXDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.barplot(x='release_day',y='views',data=freq_rel_day)\n",
        "plt.title('Most frequent release days')"
      ],
      "metadata": {
        "id": "RRuOPXvcSade"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Box plot are used to show distributions of numeric data values.especially when you want to compare them with multile groups.***"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Tuesday and thursday is the most frequent release day.***"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Here we can see the positive impact because tuseday and thurseday is the most released day.***"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "#  released week days having maximum views.\n",
        "pop_rel_day=df[['release_day','views']].groupby('release_day').agg({'views' : 'mean'})\n",
        "pop_rel_day=pop_rel_day.sort_values('views',ascending=False)\n",
        "pop_rel_day.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,10))\n",
        "plt.title('Released week days having maximum views')\n",
        "sns.barplot(x='release_day',y='views',data=pop_rel_day)"
      ],
      "metadata": {
        "id": "t0ltW8EOSskT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Box plot are used to show distributions of numeric data values.especially when you want to compare them with multile groups.***"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Friday and wednesday is the week day which has more views.***"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***After follow chart I came know , most people like to see videos on week day.***"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# month having most frequent release.\n",
        "#Encoding of release_month column with corresponding month name\n",
        "month_dict={1:'Jan',2:'Feb',3:'March',4:'April',5:'May',6:'June',7:'July',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'}\n",
        "df['release_month']=df['release_month'].map(month_dict)"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check months having maximum release\n",
        "freq_rel_month=df[['release_month','views']].groupby('release_month').agg({'views' : 'count'})\n",
        "freq_rel_month=freq_rel_month.sort_values('views',ascending=False)\n",
        "freq_rel_month.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "7EQUVE1pS-pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "sns.barplot(x='release_month',y='views',data=freq_rel_month)\n",
        "plt.title('Most frequent release months')"
      ],
      "metadata": {
        "id": "i8pqCjpkTBbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It shows which groups are highest and how other groups compare against the others.**"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Most videos released in April,March,February.**"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***It shows comparison with other month,and as per chrt we get positive idea on viwers requiments.***"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "#months having maximum average views.\n",
        "#Checking the most popular release month according to average Views\n",
        "pop_rel_month=df[['release_month','views']].groupby('release_month').agg({'views' : 'mean'})\n",
        "pop_rel_month=pop_rel_month.sort_values('views',ascending=False)\n",
        "pop_rel_month.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title('Released months having maximum average views')\n",
        "sns.barplot(x='release_month',y='views',data=pop_rel_month)"
      ],
      "metadata": {
        "id": "KbtztXauTOUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It shows which groups are highest and how other groups compare against the others.**"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***March and May nearby same maximum views.***"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***It shows positive impact on business , here all are having good views,but we know which month having maximum views,and chart showing march and may having maximum views and we know through this data,people like to watch videos in their holiday month.***"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# most frequent release year\n",
        "freq_rel_year=df[['release_year','views']].groupby('release_year').agg({'views' : 'count'})\n",
        "freq_rel_year=freq_rel_year.sort_values('views',ascending=False)\n",
        "freq_rel_year.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Most frequenr Release Year\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.barplot(x='release_year',y='views',data=freq_rel_year)\n",
        "plt.title('Most frequent release years')"
      ],
      "metadata": {
        "id": "onGJJrhDTdZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It shows which groups are highest and how other groups compare against the others.**"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***2019 is the most frequent release year, as per the data.***"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We come know most release year and more high growth year of the business.***"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Pair Plot visualization code\n",
        "plt.figure(figsize=(10,15))\n",
        "sns.heatmap(df.corr(),annot=True,cmap=plt.cm.Accent_r)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses.***\n",
        "\n",
        "***Thus to know the correlation between all the variables along with the correlation coeficients, i used correlation heatmap.***"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Talk_id,views,comments,duration,release year all count are in place.**\n",
        "\n",
        "***2.Rest all correlation in above chart.***"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Pair Plot visualization code\n",
        "\n",
        "sns.pairplot(df, hue = 'views', diag_kind = 'kde',\n",
        "             plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'k'},\n",
        "             size = 4)"
      ],
      "metadata": {
        "id": "NxNQnX1grTYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Pairplot is a module of seaborn library which provides a high-level interface for drawing attractive and informative statistical graphics.***\n",
        "\n",
        "***The density plots on the diagonal make it easier to compare distributions between the continents than stacked bars. Changing the transparency of the scatter plots increases readability because there is considerable overlap (known as overplotting) on these figures.***"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The default pairs plot by itself often gives us valuable insights.***\n",
        "\n",
        "***It shows some correlarated and uncorreted views.***\n",
        "\n",
        "***It shows some focus points like a duration.***"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### chart- 16"
      ],
      "metadata": {
        "id": "Xj3U_gcMTzax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the most popular release years w.r.t. average Views\n",
        "pop_rel_year=df[['release_year','views']].groupby('release_year').agg({'views' : 'mean'})\n",
        "pop_rel_year=pop_rel_year.sort_values('views',ascending=False)\n",
        "pop_rel_year.reset_index(inplace=True)"
      ],
      "metadata": {
        "id": "qICC0DR9UBlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title('Released years having maximum average views')\n",
        "sns.barplot(x='release_year',y='views',data=pop_rel_year)"
      ],
      "metadata": {
        "id": "XycwArRMUG3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Release year showing maximum views eg.2006 year has more views as compare to 2014,2015.***"
      ],
      "metadata": {
        "id": "p4YI9Ugwuwnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution of views\n",
        "\n",
        "sns.distplot(df[df['views'] < 0.4e7]['views'])"
      ],
      "metadata": {
        "id": "w09qGvQMUVyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution of duration.\n",
        "\n",
        "sns.distplot(df[df['duration'] < 0.4e7]['duration'])"
      ],
      "metadata": {
        "id": "7gKWL-NcUrK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correlation\n",
        "\n",
        "ax = sns.jointplot(x='views', y='duration',data=df)"
      ],
      "metadata": {
        "id": "XlLB15KuUuel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***seaborn displot represents the overall distribution of continous data variables.***\n",
        "\n",
        "***To check the distribution of the data and then if views and duration are anywhere related.***\n",
        "\n",
        "***1)This suggest a very high average level of popularity of TED Talks.***\n",
        "\n",
        "***2)we also notice that the majority of talks have views less than 4 million.***"
      ],
      "metadata": {
        "id": "B4pIgQ14v0iO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "Y5gUqqP2ZOqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Target encoding on speaker_1 column.\n",
        "pop_speaker=df.groupby('speaker_1').agg({'views' : 'mean'}).sort_values(['views'],ascending=False)\n",
        "pop_speaker=pop_speaker.to_dict()\n",
        "pop_speaker=pop_speaker.values()\n",
        "pop_speaker=  list(pop_speaker)[0]\n",
        "df['speaker_1_avg_views']=df['speaker_1'].map(pop_speaker)"
      ],
      "metadata": {
        "id": "sBEqg71VZTyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Distribution of speaker_1_avg_views column\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(df['speaker_1_avg_views'])"
      ],
      "metadata": {
        "id": "rQZT3G3QaJmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing unique event categories.\n",
        "event_list=list(df.event.unique())\n",
        "print(event_list)"
      ],
      "metadata": {
        "id": "Hml9T9D6aNvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Target encoding on event column\n",
        "pop_event=df.groupby('event').agg({'views' : 'mean'}).sort_values(['views'],ascending=False)\n",
        "pop_event=pop_event.to_dict()\n",
        "pop_event=pop_event.values()\n",
        "pop_event=  list(pop_event)[0]\n",
        "df['event_wise_avg_views']=df['event'].map(pop_event)"
      ],
      "metadata": {
        "id": "U-W8aanRaSl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Distribution of event_wise_avg_views column\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(df['event_wise_avg_views'])"
      ],
      "metadata": {
        "id": "_veK4t0raVwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we can drop the event column now as it is of no use.\n",
        "df=df.drop(columns='event')"
      ],
      "metadata": {
        "id": "tMS1x1ZuacuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying eval function on available_lang column\n",
        "df['available_lang'] = df.apply(lambda row: eval(row['available_lang']), axis=1)"
      ],
      "metadata": {
        "id": "HFkLC0y0ahAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding new feature num_of_lang\n",
        "df['num_of_lang'] = df.apply(lambda x: len(x['available_lang']), axis=1)"
      ],
      "metadata": {
        "id": "ucAtb5Ghasw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the available_lang column\n",
        "df.drop(columns='available_lang',inplace=True)\n"
      ],
      "metadata": {
        "id": "_8UNzG8_awwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['topics'][0]"
      ],
      "metadata": {
        "id": "vnOVGd4Ta3Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying Eval funcion on topics column\n",
        "df['topics'] = df.apply(lambda row: eval(row['topics']), axis=1)"
      ],
      "metadata": {
        "id": "y3LPvW_ba63M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new feature num_of_tags\n",
        "df['num_of_tags'] = df.apply(lambda x: len(x['topics']), axis=1)"
      ],
      "metadata": {
        "id": "x3kf-SGSa9me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "#Extracting the unique topics from topics list of each talk\n",
        "unique_topics=[]\n",
        "for i in range(0,len(df)):\n",
        "  temp=df['topics'][i]\n",
        "  for ele in temp:\n",
        "    if(ele not in unique_topics):\n",
        "      unique_topics.append(ele)"
      ],
      "metadata": {
        "id": "LljlHmmebjpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the avg views with respect to each topic in another dict unique_topics_avg_view_dict\n",
        "unique_topics_avg_view_dict={}\n",
        "for topic in unique_topics:\n",
        "  temp=0\n",
        "  count=0\n",
        "  for i in range(0,len(df)):\n",
        "    temp2=df['topics'][i]\n",
        "    if(topic in temp2):\n",
        "      temp+=df['views'][i]\n",
        "      count+=1\n",
        "  unique_topics_avg_view_dict[topic]=temp//count"
      ],
      "metadata": {
        "id": "HP4cO-nFbnHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the avg views with respect to topic for each talk\n",
        "topics_wise_avg_views=[]\n",
        "for i in range(0,len(df)):\n",
        "  temp=0\n",
        "  temp_topic=df['topics'][i]\n",
        "  for ele in temp_topic:\n",
        "    temp+=unique_topics_avg_view_dict[ele]\n",
        "  \n",
        "  topics_wise_avg_views.append(temp//len(temp_topic))\n",
        "\n",
        "se = pd.Series(topics_wise_avg_views)\n",
        "df['topics_wise_avg_views'] = se.values"
      ],
      "metadata": {
        "id": "mkcw7_p3bp40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of topics_wise_avg_views\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(df['topics_wise_avg_views'])"
      ],
      "metadata": {
        "id": "4T6M0nEMbs3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature engineering on published_date_column\n",
        "#Adding new feature video_age\n",
        "df['video_age']=2021-df['release_year']"
      ],
      "metadata": {
        "id": "n_04RbzRb2ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, drop published_date column\n",
        "df=df.drop(columns='published_date')"
      ],
      "metadata": {
        "id": "ZIYef4e-cB0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#applying eval function on related_talks column\n",
        "df['related_talks'] = df.apply(lambda row: eval(row['related_talks']), axis=1)"
      ],
      "metadata": {
        "id": "MMXawe5xcGyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new feature called related_views\n",
        "df['related_views'] = 0"
      ],
      "metadata": {
        "id": "S9P2URqScKAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting the value of related_talks\n",
        "for index, row in df.iterrows():\n",
        "    id_list=list(row['related_talks'].keys())\n",
        "    temp=0\n",
        "    for i in range(len(df)):\n",
        "      if (df.loc[i,'talk_id']) in id_list:\n",
        "        temp+=df.loc[i,'views']\n",
        "\n",
        "    df.loc[index,'related_views']=temp//6"
      ],
      "metadata": {
        "id": "AkRTuDVrcMVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distplot to show the distribution of related_views column\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(df['related_views'])"
      ],
      "metadata": {
        "id": "RMGtgMvVcPHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping unimportant columns from the dataframe.\n",
        "df.drop(columns=['talk_id','title','speaker_1', 'all_speakers', 'occupations',\n",
        "       'about_speakers', 'recorded_date','topics','related_talks','transcript','description','release_year','url','native_lang'],inplace=True)"
      ],
      "metadata": {
        "id": "pTyLeMM0Xlqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now Printing total number of missing values.\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "waVZdJn0fMTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing KNNImputer library to impute nan values\n",
        "from sklearn.impute import KNNImputer\n",
        "imputer = KNNImputer()\n",
        "imputer.fit(df[['comments']])\n",
        "df[['comments']] = imputer.transform(df[['comments']])"
      ],
      "metadata": {
        "id": "E6IBuKSFXuTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see if there are any nan values in comments column\n",
        "df[['comments']].isna().sum()"
      ],
      "metadata": {
        "id": "mWtJPQoAXx-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data pre-processing"
      ],
      "metadata": {
        "id": "x9864VFcY5o3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now Replacing outliers with extreme values\n",
        "for i in ['comments','duration','num_of_lang','num_of_tags','related_views','views','speaker_1_avg_views','topics_wise_avg_views','event_wise_avg_views']:\n",
        "  Q1 = df[i].quantile(0.25)\n",
        "  Q3 = df[i].quantile(0.75)\n",
        "  IQR = Q3 - Q1\n",
        "\n",
        "  df[i]=np.where(df[i]<(Q1 - 1.5 * IQR),(Q1 - 1.5 * IQR),np.where(df[i]>(Q3 + 1.5 * IQR),(Q3 + 1.5 * IQR),df[i]))"
      ],
      "metadata": {
        "id": "k7uONGXUgvK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the outliers again\n",
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).sum()"
      ],
      "metadata": {
        "id": "bJ-B3aOIgycw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zd1-k4AVbXnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing \n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "hkaV1yc9s04R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "df['release_month']"
      ],
      "metadata": {
        "id": "30b_lR_Rs3eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "clean = re.compile('<.*>?')\n",
        "re.sub(clean, '',df.iloc[2].release_month)"
      ],
      "metadata": {
        "id": "RR5HPEtvtH4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# converting everithing into lower case\n",
        "\n",
        "def convert_lower(text):\n",
        "  return text.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['release_month']=df['release_month'].apply(convert_lower)"
      ],
      "metadata": {
        "id": "wiuoGw8_tV2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['release_month']"
      ],
      "metadata": {
        "id": "gDAlUaxCteh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "def remove_punctuation(topics):\n",
        "    '''a function for removing punctuation'''\n",
        "    import string\n",
        "    # replacing the punctuations with no space, \n",
        "    # which in effect deletes the punctuation marks \n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # return the topics stripped of punctuation marks\n",
        "    return topics.translate(translator)"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to each examples\n",
        "\n",
        "df['release_month'] = df['release_month'].apply(remove_punctuation)\n",
        "df.release_month"
      ],
      "metadata": {
        "id": "8EP8rnhQtr5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the stopwords from nltk library\n",
        "sw = stopwords.words('english')\n",
        "# displaying the stopwords\n",
        "np.array(sw)"
      ],
      "metadata": {
        "id": "PiNexnMkt50E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of stopwords: \", len(sw))"
      ],
      "metadata": {
        "id": "OxbO1t0Nt_WG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove stopwords\n",
        "\n",
        "def stopwords(release_month):\n",
        "    '''a function for removing the stopword'''\n",
        "    # removing the stop words and lowercasing the selected words\n",
        "    topics = [word.lower() for word in release_month.split() if word.lower() not in sw]\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(release_month)"
      ],
      "metadata": {
        "id": "XT6k8YHquCP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['release_month'] = df['release_month'].apply(stopwords)\n",
        "df.release_month"
      ],
      "metadata": {
        "id": "uF_ygxpVuGCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = \"computers entertainment interface design media music performance simplicity software technology\""
      ],
      "metadata": {
        "id": "WDSQRhZ0u7cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "print(topics.replace(' ',''))"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Tokenization\n",
        "topics = \"computers entertainment interface design media music performance simplicity software technology\"\n",
        "topics.split(' ')"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "#Separating the dependent(y) and independent(X) variables\n",
        "X=df.drop(columns='views')\n",
        "y=df['views']"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoding on independent features.\n",
        "X=pd.get_dummies(X)\n",
        "X.shape"
      ],
      "metadata": {
        "id": "o6ZLxkDLg9Cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating f scores of each features\n",
        "f_scores = f_regression(X,y)"
      ],
      "metadata": {
        "id": "xn-VEenZhAvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot the f scores of independent variable\n",
        "p_values= pd.Series(f_scores[1],index= X.columns)\n",
        "p_values.plot(kind='bar',color='blue',figsize=(18,8))\n",
        "plt.title('P-value scores for numerical features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vFzGR4_dju_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Let's make a list of important features.\n",
        "X=X[['comments', 'duration', 'num_of_lang','event_wise_avg_views','related_views', 'release_day_Friday','speaker_1_avg_views','topics_wise_avg_views','release_day_Saturday','release_day_Sunday']]\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the correlation matrix\n",
        "#Correlation matrix of numerical features.\n",
        "fig = plt.figure(figsize=(16,10),edgecolor='k',facecolor='xkcd:light green')\n",
        "ax = fig.add_subplot(111)\n",
        "sns.heatmap(X.corr(),annot=True, cmap='YlGnBu')"
      ],
      "metadata": {
        "id": "PB4XsAN4kVgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Here some high correlation and some low correlation also there.***\n",
        "\n",
        "***speaker_1_avg_views and topic_wise_avg_views are nearby same.***\n"
      ],
      "metadata": {
        "id": "i7Pbwsl7kHuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "KMYqtw8Kkj18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Scaling your data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "***Answer***\n",
        "\n",
        "***The train_test_split () method is used to split our data into train and test sets. First, we need to divide our data into features (X) and labels (y). The dataframe gets divided into X_train,X_test , y_train and y_test. X_train and y_train sets are used for training and fitting the model.***"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why? "
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "rf = RandomForestRegressor(criterion='mae')\n",
        "# Fit the Algorithm\n",
        "rf.fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "y_pred = rf.predict(X_train)\n",
        "y_test_pred = rf.predict(X_test)\n",
        "print('Training MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_train, y_pred)))\n",
        "print('Test MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_test, y_test_pred)))"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest Regressor hyperparameters.\n",
        "\n",
        "# Number of trees\n",
        "n_estimators = [80,100,120,150]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,80,100]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [10,30,40,50]\n",
        "\n",
        "# HYperparameter Dict\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}"
      ],
      "metadata": {
        "id": "rFOtZhrwlAff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "rf_model = RandomForestRegressor(criterion='mae')\n",
        "rf_random = RandomizedSearchCV(rf_model,param_dict,verbose=0,cv=5)\n",
        "# Fit the Algorithm\n",
        "rf_random.fit(X_train,y_train)\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see which set of hyperparameters are optimal\n",
        "rf_optimal_model = rf_random.best_estimator_\n",
        "rf_optimal_model"
      ],
      "metadata": {
        "id": "zzr_2LNKlR6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the MAE on train and test set\n",
        "y_pred = rf_optimal_model.predict(X_train)\n",
        "y_test_pred = rf_optimal_model.predict(X_test)\n",
        "print('Training MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_train, y_pred)))\n",
        "print('Test MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_test, y_test_pred)))"
      ],
      "metadata": {
        "id": "ITG4EfsHnQ2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions for test data and also calculating r2_score\n",
        "y_hat = rf_optimal_model.predict(X_test)\n",
        "print(f'R_squared value for train: {rf_optimal_model.score(X_train, y_train)}')\n",
        "r_squared= r2_score(y_test,y_hat)\n",
        "\n",
        "\n",
        "#Calculating Adjusted R-sqaured\n",
        "adjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
        "print(f'For test set the R_Squared for RFforest is {r_squared} and adjusted R_Squared is {adjusted_r_squared}')"
      ],
      "metadata": {
        "id": "_vcMy3qYnT_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's store all the scores in a dataframe\n",
        "rf=RandomForestRegressor(criterion='mae',max_depth=6,min_samples_leaf=30,min_samples_split=80,n_estimators=150,random_state=42)\n",
        "rf_optimal_model=rf.fit(X_train,y_train)\n",
        "model_data=[]\n",
        "model_dict={}\n",
        "model_dict[\"Model_Name\"] = 'RandomForestRegressor'\n",
        "model_dict[\"MAE_train\"] =mean_absolute_error(y_train, rf_optimal_model.predict(X_train))\n",
        "model_dict[\"MAE_test\"] =mean_absolute_error(y_test, rf_optimal_model.predict(X_test))\n",
        "model_dict[\"R2_Score_train\"] = r2_score(y_train,rf_optimal_model.predict(X_train))\n",
        "model_dict[\"R2_Score_test\"] = r2_score(y_test,rf_optimal_model.predict(X_test))\n",
        "model_dict[\"RMSE_train\"] = np.sqrt(mean_squared_error(y_train,rf_optimal_model.predict(X_train)))\n",
        "model_dict[\"RMSE_test\"] = np.sqrt(mean_squared_error(y_test,rf_optimal_model.predict(X_test)))\n",
        "model_data.append(model_dict)\n",
        "\n",
        "results_df_rf = pd.DataFrame(model_data)"
      ],
      "metadata": {
        "id": "P169cG1EnXms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the results_df_rf\n",
        "results_df_rf"
      ],
      "metadata": {
        "id": "X64ltehxnbTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature importance score w.r.t. RF model\n",
        "# Plotting Barplot showing important features w.r.t. RF model\n",
        "importances = pd.DataFrame({'Features': X.columns, \n",
        "                                'Importances': rf_optimal_model.feature_importances_})\n",
        "    \n",
        "importances.sort_values(by=['Importances'], axis='index', ascending=False, inplace=True)\n",
        "fig = plt.figure(figsize=(14, 6))\n",
        "sns.barplot(x='Features', y='Importances', data=importances)\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.title('Feature importance score w.r.t. RFRegressor model')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Yxo9pdqfnhsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***I perform RandomizedSearch CV definining Criterian mode with MAE(mean absoulte eroor).***\n",
        "\n",
        "***The CV stands for cross-validation.***"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** speaker_1_avg_views is showing high barplot of RandomForestregressor.***"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "#XGBoost Regressor hyperparameters\n",
        "\n",
        "# Number of trees\n",
        "n_estimators = [30,50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [30,40,50]\n",
        "\n",
        "# HYperparameter Dict\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "xgb_model = XGBRegressor(learning_rate=0.1)\n",
        "# Fit the Algorithm\n",
        "xgb_random = RandomizedSearchCV(xgb_model,param_dict,verbose=2,cv=5)\n",
        "xgb_random.fit(X_train,y_train)\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see which set of hyperparameters are optimal\n",
        "xgb_optimal_model = xgb_random.best_estimator_\n",
        "xgb_optimal_model"
      ],
      "metadata": {
        "id": "IHz1bIrcpg_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions for test data and also calculating r2_score.\n",
        "\n",
        "y_hat = xgb_optimal_model.predict(X_test)\n",
        "print(f'r_squared value for train: {xgb_optimal_model.score(X_train, y_train)}')\n",
        "r_squared= r2_score(y_test,y_hat)\n",
        "\n",
        "# Calculating Adjusted R-sqaured\n",
        "\n",
        "adjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
        "print(f'For test set the R_Squared for XGBoost is {r_squared} and adjusted R_Squared is {adjusted_r_squared}')"
      ],
      "metadata": {
        "id": "4s7fIdbMpkck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating MAE for training and test set.\n",
        "y_pred = xgb_optimal_model.predict(X_train)\n",
        "y_test_pred = xgb_optimal_model.predict(X_test)\n",
        "print('Training MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_train, y_pred)))\n",
        "print('Test MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_test, y_test_pred)))"
      ],
      "metadata": {
        "id": "1oNNG40mpnSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing all the scores in a dataframe\n",
        "xgb=XGBRegressor(base_score=0.5, booster='gbtree',colsample_bytree=1, gamma=0,importance_type='gain', learning_rate=0.1,max_depth=3, min_samples_leaf=40,\n",
        "             min_samples_split=40, n_estimators=40)\n",
        "xgb_optimal_model=xgb.fit(X_train,y_train)\n",
        "model_data=[]\n",
        "model_dict={}\n",
        "model_dict[\"Model_Name\"] = 'XGBRegressor'\n",
        "model_dict[\"MAE_train\"] =mean_absolute_error(y_train, xgb_optimal_model.predict(X_train))\n",
        "model_dict[\"MAE_test\"] =mean_absolute_error(y_test, xgb_optimal_model.predict(X_test))\n",
        "model_dict[\"R2_Score_train\"] = r2_score(y_train,xgb_optimal_model.predict(X_train))\n",
        "model_dict[\"R2_Score_test\"] = r2_score(y_test,xgb_optimal_model.predict(X_test))\n",
        "model_dict[\"RMSE_train\"] = np.sqrt(mean_squared_error(y_train,xgb_optimal_model.predict(X_train)))\n",
        "model_dict[\"RMSE_test\"] = np.sqrt(mean_squared_error(y_test,xgb_optimal_model.predict(X_test)))\n",
        "model_data.append(model_dict)\n",
        "\n",
        "results_df_xgb = pd.DataFrame(model_data)"
      ],
      "metadata": {
        "id": "rSvsfD3WpqKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the results_df_xgb\n",
        "results_df_xgb"
      ],
      "metadata": {
        "id": "sTO5D2typs1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot the feature importance score w.r.t. XGBRegressor model\n",
        "#Barplot showing important features w.r.t. XGBregressor model\n",
        "importances = pd.DataFrame({'Features': X.columns, \n",
        "                                'Importances': xgb_optimal_model.feature_importances_})\n",
        "    \n",
        "importances.sort_values(by=['Importances'], axis='index', ascending=False, inplace=True)\n",
        "fig = plt.figure(figsize=(14, 4))\n",
        "sns.barplot(x='Features', y='Importances', data=importances)\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.title('Feature importance score w.r.t. XGBregressor model')\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "GdU-vOZ7pxBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The XGBoost is a popular supervised machine learning model with characteristics like computation speed, parallelization, and performance.***\n",
        "\n",
        "***XGBoost the Algorithm learns a model faster than many other machine learning models and works well on categorical data and limited datasets***\n",
        "\n",
        "***In RandomizedSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.***\n",
        "\n",
        "***That's why I have used RandomizedSearchCV method for hyperparameter optimization.***"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Speaaker_1_avg_views showing high score as per the chart.***\n",
        "\n",
        "***index\tModel_Name-XGBRegressor,\tMAE_train-211051.556655,\tMAE_test-228812.768108,\tR2_Score_train-0.866211,\tR2_Score_test-0.832566,\tRMSE_train-403273.960146,\tRMSE_test-451028.530993***\n",
        "\n",
        "*** For testing dataset,I found above data.***"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Evaluation metrics are used to measure the quality of the model.***"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extra Trees Regressor hyperparameters tuning\n",
        "\n",
        "# Number of trees\n",
        "n_estimators = [50,80,100]\n",
        "\n",
        "# Maximum depth of trees\n",
        "max_depth = [4,6,8]\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [50,100,150]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [40,50]\n",
        "\n",
        "# HYperparameter Dict\n",
        "param_dict = {'n_estimators' : n_estimators,\n",
        "              'max_depth' : max_depth,\n",
        "              'min_samples_split' : min_samples_split,\n",
        "              'min_samples_leaf' : min_samples_leaf}"
      ],
      "metadata": {
        "id": "YbNdVCJ0p5s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "5gamfAt-jCo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross- Validation & Hyperparameter Tuning.\n",
        "\n",
        "# ML Model - 3 Implementation\n",
        "et_model = ExtraTreesRegressor(criterion='mae')\n",
        "# Fit the Algorithm\n",
        "et_random = RandomizedSearchCV(et_model,param_dict,verbose=2,cv=5)\n",
        "# Predict on the model\n",
        "et_random.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see optimal set of hyperparameters for this model\n",
        "et_optimal_model = et_random.best_estimator_\n",
        "et_optimal_model"
      ],
      "metadata": {
        "id": "1npBFPhDqI4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating MAE for training and test set\n",
        "y_pred = et_optimal_model.predict(X_train)\n",
        "y_test_pred = et_optimal_model.predict(X_test)\n",
        "print('Training MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_train, y_pred)))\n",
        "print('Test MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_test, y_test_pred)))"
      ],
      "metadata": {
        "id": "y66w9gRyqKM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = et_optimal_model.predict(X_train)\n",
        "y_test_pred = et_optimal_model.predict(X_test)\n",
        "print('Training MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_train, y_pred)))\n",
        "print('Test MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_test, y_test_pred)))"
      ],
      "metadata": {
        "id": "Z2NAPVOYqPjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions for test data and also calculating r2_score\n",
        "\n",
        "y_hat = et_optimal_model.predict(X_test)\n",
        "print(f'r_squared value for train: {rf_optimal_model.score(X_train, y_train)}')\n",
        "r_squared= r2_score(y_test,y_hat)\n",
        "\n",
        "#Calculate Adjusted R-sqaured\n",
        "\n",
        "adjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
        "print(f'For test the R_Squared for ExtraTreesRegressor is {r_squared} and adjusted R_Squared is {adjusted_r_squared}')"
      ],
      "metadata": {
        "id": "8MIiw6zqqQQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's store all the scores in a dataframe\n",
        "et=ExtraTreesRegressor(criterion='mae',max_depth=8, min_samples_leaf=50,min_samples_split=100, n_estimators=50)\n",
        "et_optimal_model=et.fit(X_train,y_train)\n",
        "model_data=[]\n",
        "model_dict={}\n",
        "model_dict[\"Model_Name\"] = 'ExtraTreeRegressor'\n",
        "model_dict[\"MAE_train\"] =mean_absolute_error(y_train, et_optimal_model.predict(X_train))\n",
        "model_dict[\"MAE_test\"] =mean_absolute_error(y_test, et_optimal_model.predict(X_test))\n",
        "model_dict[\"R2_Score_train\"] = r2_score(y_train,et_optimal_model.predict(X_train))\n",
        "model_dict[\"R2_Score_test\"] = r2_score(y_test,et_optimal_model.predict(X_test))\n",
        "model_dict[\"RMSE_train\"] = np.sqrt(mean_squared_error(y_train,et_optimal_model.predict(X_train)))\n",
        "model_dict[\"RMSE_test\"] = np.sqrt(mean_squared_error(y_test,et_optimal_model.predict(X_test)))\n",
        "model_data.append(model_dict)\n",
        "\n",
        "results_df_et = pd.DataFrame(model_data)"
      ],
      "metadata": {
        "id": "_jFVJ7JRqVhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df_et"
      ],
      "metadata": {
        "id": "mnxuQ8kAqYJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Barplot showing important features w.r.t. ExtraTrees Regressor model.\n",
        "importances = pd.DataFrame({'Features': X.columns, \n",
        "                                'Importances': et.feature_importances_})\n",
        "    \n",
        "importances.sort_values(by=['Importances'], axis='index', ascending=False, inplace=True)\n",
        "fig = plt.figure(figsize=(14, 4))\n",
        "sns.barplot(x='Features', y='Importances', data=importances)\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.title('Feature importance score w.r.t. ExtraTreesRegressor model')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-vBnvYA6qdyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Let's define all these models\n",
        "models = [\n",
        "           ['RandomForest ',RandomForestRegressor(criterion='mae',max_depth=6,min_samples_leaf=30,min_samples_split=80,n_estimators=150,random_state=42)],\n",
        "           ['ExtraTreeRegressor :',ExtraTreesRegressor(criterion='mae',max_depth=8, min_samples_leaf=50,min_samples_split=100, n_estimators=50)],\n",
        "           ['XGBRegressor: ', XGBRegressor(base_score=0.5, booster='gbtree',colsample_bytree=1, gamma=0,importance_type='gain', learning_rate=0.1,max_depth=3, min_samples_leaf=40,\n",
        "             min_samples_split=40, n_estimators=40)]        \n",
        "        ]"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's run all the models and store the scores\n",
        "model_data = []\n",
        "for name,curr_model in models :\n",
        "    model_dict = {}\n",
        "    curr_model.random_state = 78\n",
        "    model_dict[\"Model_Name\"] = name\n",
        "    curr_model.fit(X_train,y_train)\n",
        "    model_dict[\"MAE_train\"] =metrics.mean_absolute_error(y_train, curr_model.predict(X_train))\n",
        "    model_dict[\"MAE_test\"] =metrics.mean_absolute_error(y_test, curr_model.predict(X_test))\n",
        "    model_dict[\"R2_Score_train\"] = r2_score(y_train,curr_model.predict(X_train))\n",
        "    model_dict[\"R2_Score_test\"] = r2_score(y_test,curr_model.predict(X_test))\n",
        "    model_dict[\"RMSE_Score_train\"] = np.sqrt(mean_squared_error(y_train,curr_model.predict(X_train)))\n",
        "    model_dict[\"RMSE_Score_test\"] = np.sqrt(mean_squared_error(y_test,curr_model.predict(X_test)))\n",
        "    model_data.append(model_dict)"
      ],
      "metadata": {
        "id": "PgAvJG9brmU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now Printing the results dataframe\n",
        "comparision_df = pd.DataFrame(model_data)\n",
        "comparision_df"
      ],
      "metadata": {
        "id": "C2IuYduVrp3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Final selection of the model\n",
        "# Let's print the scores of Random Forest Regressor model\n",
        "results_df_rf"
      ],
      "metadata": {
        "id": "-37Xck3xrsz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing mean value of target variable.\n",
        "print(f'Mean value of our target variable is {y.mean()}')"
      ],
      "metadata": {
        "id": "b0mWD-QAry9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Randomized search on hyper parameters.***\n",
        "\n",
        "***Here i am plotting the feature importance score with respect to ExtraTreesRegressor model***\n",
        "\n",
        "***I have used randomizedsearchCV giving et_model with criterian MAE.***\n",
        "\n",
        "***The advantage of this method is that there is a greater chance of finding regions of the cost minimization space with more suitable hyperparameters***"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***speaker_1_avg_views is impacting the much.***"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***For Random Forest Regressor model, MAE is around 10 % of target variable mean.***\n",
        "\n",
        "***We choose MAE and not RMSE as the deciding factor of our model selection because of the following reasons:***\n",
        "\n",
        "***RMSE is heavily influenced by outliers as in the higher the values get the more the RMSE increases.***\n",
        "\n",
        "***MAE doesn’t increase with outliers. MAE is linear and RMSE is quadratically increasing.***\n",
        "\n",
        "***The best performing regressor model for this dataset is Random Forest Regressor on the basis of MAE.***"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The ML regressor models that i am using are:***\n",
        "\n",
        "***1)Random Forest Regressor***\n",
        "\n",
        "***2)Extra Tree Regressor***\n",
        "\n",
        "***3)XGB Regressor***"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Text classification is a fundamental machine learning problem/programme with a set of applications of various data accorss the text predictions. In this setup, we just broken down in to the text classification workflow into several steps. For each step, we had suggest a customized approach based on the characteristics of  specific dataset and its all respects. It content the set of ratio's having number of samples to the number of words., I also suggest a model type, which will gets you closer to the same and having a best performance with quick access.***\n",
        "\n",
        "***The other steps are , the accompanying code, and the flowchart will help you to understand,my best results.***\n",
        "\n",
        "***1) From EDA we see that here, dataset has 4005 rows and 19 columns.***\n",
        "\n",
        "***2) zero(0) duplicate values.***\n",
        "\n",
        "***3)There are 5 columns that have mising values.***\n",
        "\n",
        "***4) comments, duration, talk id, and views this data has huge outliners,i work on those outliners.***\n",
        "\n",
        "***5) I applied strtime on publish date,this colum has very important role in dataset.***\n",
        "\n",
        "***6)RandomForestRegressor is the best performer in terms of MAE.***\n",
        "\n",
        "***7)In all the features speaker_wise_avg_views is most important this implies that speakers are directly impacting the views.***"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}